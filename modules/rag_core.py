# modules/rag_core.py

from itertools import chain
from modules.vector_store import load_index
from modules.embedder import get_embedding_model
from modules.llm_groq import get_llm
from modules.prompt_template import get_proposal_prompt_template
from modules.reranker import rerank
from langchain.schema import Document
from typing import List, Tuple
from langchain.chains import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain

"""def full_rag_pipeline(query: str, index_path: str = "vector_store/propales_index", faiss_k: int = 20, final_k: int = 4) -> Tuple[str, List[Tuple[Document, float]]]:
    
    #Pipeline complet : recherche FAISS ‚Üí reranking ‚Üí g√©n√©ration LLaMA3
    #Retourne : texte g√©n√©r√© + documents rerank√©s (avec score)
        # 1. Load index
    embedder = get_embedding_model()
    index = load_index(index_path, embedder)

    # 2. Recherche initiale FAISS
    retrieved_docs = index.similarity_search(query, k=faiss_k) # type: ignore
    print(f"üîç {len(retrieved_docs)} documents r√©cup√©r√©s par FAISS.")

    # 3. Reranking CrossEncoder
    reranked_docs = rerank(query, retrieved_docs, top_k=final_k)
    print(f" {len(reranked_docs)} documents apr√®s reranking.")

    # 4. G√©n√©ration via StuffDocumentsChain
    docs = [doc for doc, _ in reranked_docs]

    llm = get_llm("llama3-8b-8192")
    prompt = get_proposal_prompt_template()

    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True)
    chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="context", verbose=True)
    print("üßæ Nombre de documents inject√©s dans le prompt :", len(docs))
    for i, doc in enumerate(docs):
        print(f"Document {i+1} - {len(doc.page_content)} caract√®res - Source: {doc.metadata.get('source', 'N/A')}")

    result = chain.run({"input_documents": docs, "question": query})

    return result, reranked_docs
""" 
def full_rag_pipeline(query: str, index_path: str = "vector_store/propales_index", faiss_k: int = 20, final_k: int = 4) -> Tuple[str, List[Tuple[Document, float]]]:
    """
    Pipeline complet : recherche FAISS ‚Üí reranking ‚Üí g√©n√©ration LLaMA3 (avec contr√¥le du contexte)
    """
    from modules.vector_store import load_index
    from modules.embedder import get_embedding_model
    from modules.llm_groq import get_llm
    from modules.prompt_template import get_proposal_prompt_template
    from modules.reranker import rerank
    from langchain.chains import LLMChain

    # 1. Index + Recherche
    embedder = get_embedding_model()
    index = load_index(index_path, embedder)

    retrieved_docs = index.similarity_search(query, k=faiss_k)
    print(f"üîç {len(retrieved_docs)} documents r√©cup√©r√©s par FAISS.")

    reranked_docs = rerank(query, retrieved_docs, top_k=final_k)
    print(f"üèÖ {len(reranked_docs)} documents apr√®s reranking.")

    # 2. Pr√©paration du contexte
    docs = [doc for doc, _ in reranked_docs]

    context = "\n\n".join(doc.page_content.strip() for doc in docs)
    print(f"üßæ Longueur totale du contexte : {len(context)} caract√®res")

    # Option : tronquer si trop long (selon ton mod√®le)
    MAX_CONTEXT_CHARS = 12000
    if len(context) > MAX_CONTEXT_CHARS:
        print(f"‚ö†Ô∏è Contexte tronqu√© √† {MAX_CONTEXT_CHARS} caract√®res.")
        context = context[:MAX_CONTEXT_CHARS]

    # 3. Appel du mod√®le avec prompt template
    prompt = get_proposal_prompt_template()
    llm = get_llm("llama3-8b-8192")

    chain = LLMChain(llm=llm, prompt=prompt, verbose=True)
    result = chain.run({"context": context, "question": query})

    return result, reranked_docs
