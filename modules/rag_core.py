# modules/rag_core.py
from itertools import chain
from modules.vector_store import load_index
from modules.embedder import get_embedding_model
from modules.llm_openai import get_llm
from modules.prompt_template import get_proposal_prompt_template
from modules.reranker import rerank
from langchain.schema import Document
from typing import List, Tuple
from langchain.chains import LLMChain

def full_rag_pipeline(query: str, index_path: str = "vector_store/propales_index", faiss_k: int = 20, final_k: int = 4) -> Tuple[str, List[Tuple[Document, float]]]:
    """
    Pipeline complet : recherche FAISS ‚Üí reranking ‚Üí g√©n√©ration LLaMA3 (avec contr√¥le du contexte)
    """


    # 1. Index + Recherche
    embedder = get_embedding_model()
    index = load_index(index_path, embedder)

    retrieved_docs = index.similarity_search(query, k=faiss_k)
    print(f"üîç {len(retrieved_docs)} documents r√©cup√©r√©s par FAISS.")

    reranked_docs = rerank(query, retrieved_docs, top_k=final_k)
    print(f"üèÖ {len(reranked_docs)} documents apr√®s reranking.")

    # 2. Pr√©paration du contexte
    docs = [doc for doc, _ in reranked_docs]

    context = "\n\n".join(doc.page_content.strip() for doc in docs)
    print(f"üßæ Longueur totale du contexte : {len(context)} caract√®res")

    # Option : tronquer si trop long (selon ton mod√®le)
    MAX_CONTEXT_CHARS = 12000
    if len(context) > MAX_CONTEXT_CHARS:
        print(f"‚ö†Ô∏è Contexte tronqu√© √† {MAX_CONTEXT_CHARS} caract√®res.")
        context = context[:MAX_CONTEXT_CHARS]

    # 3. Appel du mod√®le avec prompt template
    prompt = get_proposal_prompt_template()
    llm = get_llm("gpt-3.5-turbo")

    chain = LLMChain(llm=llm, prompt=prompt, verbose=True)
    result = chain.run({"context": context, "question": query})

    return result, reranked_docs